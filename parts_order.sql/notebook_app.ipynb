{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "4f8aa7e2-ae03-420e-b38f-2f93e8a3e3cb",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "-- Switch to the role with administrative privileges\n-- The 'accountadmin' role is required to perform account-level operations such as creating databases and tables.\nuse role accountadmin;\n\n-- Create a new database named 'snowpipe_demo'\n-- This command creates a new database in Snowflake where data can be stored and managed. The 'snowpipe_demo' database will be used for the demonstration or experimentation purposes related to Snowpipe or other data ingestion methods.\ncreate or replace database snowpipe_demo;\n\n-- Create a new table named 'orders_data_lz'\n-- The 'orders_data_lz' table is used to store order-related information for a manufacturing or e-commerce business.\n-- The structure of the table includes the following columns:\n--   - order_id: A unique identifier for each order (integer type).\n--   - product: The name or identifier of the product being ordered (string type with a maximum length of 20 characters).\n--   - quantity: The number of units of the product being ordered (integer type).\n--   - order_status: The current status of the order (e.g., \"Pending\", \"Shipped\", \"Delivered\") (string type with a maximum length of 30 characters).\n--   - order_date: The date the order was placed (date type).\ncreate or replace table orders_data_lz(\n    order_id int,                  -- Integer type for the order identifier\n    product varchar(20),           -- String type with a max length of 20 for product name/ID\n    quantity int,                  -- Integer type for the quantity of the product ordered\n    order_status varchar(30),      -- String type with a max length of 30 for order status\n    order_date date                -- Date type to store the date the order was placed\n);\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "source": "-- Create a Cloud Storage Integration in Snowflake\n-- This integration allows Snowflake to securely access an external cloud storage location.\n-- It creates a configuration that grants Snowflake the necessary permissions to read from the specified GCS bucket.\ncreate or replace storage integration gcs_bucket_read_int\n type = external_stage             -- Type of the integration, external stage indicates access to an external location.\n storage_provider = gcs            -- Specifies the cloud storage provider (Google Cloud Storage in this case).\n enabled = true                    -- Enables the integration after creation.\n storage_allowed_locations = ('gcs://data_for_snowpipe_demo/');  -- Specifies the allowed location(s) in the external cloud storage.\n\n-- Optional: Drop integration if needed\n-- drop integration gcs_bucket_read_int;\n\n-- Retrieve the Cloud Storage Service Account for your Snowflake account\n-- The 'desc' command provides metadata and details about the created storage integration.\ndesc storage integration gcs_bucket_read_int;\n\n-- Service account info for storage integration\n-- This is the service account associated with the storage integration.\n-- kkni00000@gcpuscentral1-1dfa.iam.gserviceaccount.com\n\n-- A stage in Snowflake refers to an external or internal location where data is stored before being loaded into Snowflake tables.\n-- Here, a stage is created for accessing data stored in GCS.\ncreate or replace stage snowpipe_stage\n  url = 'gcs://data_for_snowpipe_demo/'    -- URL of the external GCS bucket to be used for data ingestion.\n  storage_integration = gcs_bucket_read_int;  -- The storage integration created earlier, providing access to GCS.\n\n-- Show stages to list all available stages\nshow stages;\n\n-- List files in the specified stage, useful for verifying data availability\nlist @snowpipe_stage;\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4a0caa63-a43b-4c65-a2d7-94edc3892cf8",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "-- Create PUB-SUB Topic and Subscription\n-- Google Cloud Storage events are sent to a Pub/Sub topic. This command triggers event notifications.\n-- Example command: gsutil notification create -t snowpipe_pubsub_topic -f json gs://data_for_snowpipe_demo/\n\n-- Create a notification integration that connects Snowflake to the GCP Pub/Sub system for event-driven data ingestion\ncreate or replace notification integration notification_from_pubsub_int\n type = queue                         -- Type of the integration; queue indicates a notification system using Pub/Sub.\n notification_provider = gcp_pubsub    -- Specifies that Google Cloud Pub/Sub is used for notifications.\n enabled = true                        -- Enables the integration after creation.\n gcp_pubsub_subscription_name = 'projects/amiable-anagram-446201-h1/subscriptions/snowpipe_pubsub_topic-sub';  -- GCP Pub/Sub subscription name to listen for notifications.\n\n-- Describe the created notification integration to check its details\ndesc integration notification_from_pubsub_int;\n\n-- Service account for PUB-SUB\n-- The service account associated with the Pub/Sub notification integration.\n-- kypi00000@gcpuscentral1-1dfa.iam.gserviceaccount.com",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd1cd279-3eb7-4786-b78d-1ab6519ea4ca",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "-- Create Snow Pipe to load data automatically into Snowflake when new files arrive in the external stage\n-- Snowpipe is a serverless, continuous data ingestion service in Snowflake. It loads data automatically based on notifications from the cloud storage or other event sources.\nCreate or replace pipe gcs_to_snowflake_pipe\nauto_ingest = true                             -- Automatically ingest data from the stage when new files arrive.\nintegration = notification_from_pubsub_int     -- Use the notification integration created earlier to trigger data loading.\nas\ncopy into orders_data_lz                      -- The target table where data will be loaded.\nfrom @snowpipe_stage                           -- The external stage that contains the data to be ingested.\nfile_format = (type = 'CSV');                  -- Specifies that the file format is CSV.\n\n-- Show pipes to list all existing pipes in Snowflake\nshow pipes;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "563ba787-634c-4181-8cdf-2d2018c7aa46",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "-- Check the status of the pipe to ensure data loading is working correctly\nselect system$pipe_status('gcs_to_snowflake_pipe');\n\n-- Retrieve historical information about the copy operations performed by Snowpipe\nSelect * \nfrom table(information_schema.copy_history(table_name=>'orders_data_lz', start_time=> dateadd(hours, -1, current_timestamp())));\n\n-- Check the current contents of the target table\nselect * from orders_data_lz;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1632ac2-1a11-4c35-90a1-30a27f8f2800",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\n-- Start Snowpipe to resume automatic ingestion of new files\nALTER PIPE gcs_to_snowflake_pipe SET PIPE_EXECUTION_PAUSED = false;\n\n-- Stop Snowpipe temporarily to pause automatic ingestion\nALTER PIPE gcs_to_snowflake_pipe SET PIPE_EXECUTION_PAUSED = true;\n\n\n-- Terminate (drop) the Snowpipe once it is no longer needed\ndrop pipe gcs_snowpipe;",
   "execution_count": null
  }
 ]
}